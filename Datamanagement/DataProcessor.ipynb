{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4e78d7",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b447db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import done\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Ontology\n",
    "#!pip install owlready2 \n",
    "#import owlready2 as owl\n",
    "from owlready2 import *\n",
    "import re  # To separate words based on capital letters in onto classes & to split search queries\n",
    "#!pip install EMMOntoPy #Special EMMO package\n",
    "# from ontopy import get_ontology\n",
    "print(\"Import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0187ae1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343b14ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your imported ontology with 33 classes is ready to use\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Import Ontology & Select classes\"\"\"\n",
    "\n",
    "# write the location to the ontology to the onto_path list\n",
    "onto_path = [\"https://raw.githubusercontent.com/hendelhendel/FAIR_Battery/main/Ontology/test2.owl\"]\n",
    "    # Note that all the ontology files has to be owl files.\n",
    "#onto_path = [\"https://raw.githubusercontent.com/hendelhendel/FAIR_Battery/main/Ontology/flowtest.owl\"]\n",
    "#onto_path = [\"https://github.com/hendelhendel/FAIR_Battery/blob/main/Ontology/flowbatterytest.owl\"]\n",
    "    \n",
    "# Import ontology\n",
    "try:\n",
    "    onto = get_ontology(onto_path[0]).load()\n",
    "except:\n",
    "    pass \n",
    "\n",
    "onto = get_ontology(onto_path[0]).load()\n",
    "\n",
    "# Collecting classes from ontology in a list\n",
    "class_raw = list(get_ontology(onto_path[0]).load().classes())\n",
    "\n",
    "# select classes by Prefix, suffix, nametags\n",
    "tag = 'ElectrochemicalFlowCell'\n",
    "prefix = 'electrochemistry.'\n",
    "\n",
    "ClassCleaner = lambda x : re.sub('_',  ' ',\\\n",
    "                                 re.sub(r\"(?<=\\w)([A-Z])\", r\" \\1\", \\\n",
    "                                 str(x).removesuffix(tag).removeprefix(prefix))) \\\n",
    "                                    if (str(x).find(prefix) != -1) else \"!EMPTY CLASS\" \n",
    "                            \n",
    "\n",
    "class_select = list(map(ClassCleaner, filter(lambda x : (str(x).find(tag) != -1), class_raw)))\n",
    "\n",
    "print(\"Your imported ontology with \"  + str(len(class_select)) + \" classes is ready to use\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e21505",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993b26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import Zotero Data Base from Github\"\"\"\n",
    "\n",
    "# Import raw data from github repository as dataframe\n",
    "data_path = 'https://raw.githubusercontent.com/SanliFaez/FAIR-Battery-knowledgebase/main/Datamanagement/Data_Raw.csv'\n",
    "df_raw = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2047fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Process Functions\"\"\"\n",
    "\n",
    "# Function Text Cleaner\n",
    "f_CleanText = lambda text : re.split(\"\\. |\\! |\\? \", text.lower()) if (type(text) == str)  else [\"Not Available\"]\n",
    "    # Cleans text split scentices.\n",
    "    # Output is of strings per for every text inputed. \n",
    "    # Example: Input = sentence1.sentence2!sentence3? --> Output = [sentence1,sentence2,sentence3]\n",
    "    \n",
    "    \n",
    "# Function Keyword Search    \n",
    "f_SearchKeyword = lambda text, keyword : text if (text.find(keyword) != -1) else None \n",
    "    # Searches text for keyword\n",
    "    # Input: text = string or list of strings, keyword = string or list strings\n",
    "    # Example: \n",
    "        # text = [sentence1,sentence2,sentence_A,sentence_B]\n",
    "        # keyword = len(text)*[A]\n",
    "        # Output  = [0,0,sentence_A,0]\n",
    "        \n",
    "# Function Scan lists of lists with text\n",
    "f_ScanList = lambda text, keyword: list(map(f_SearchKeyword, text , len(text)*[keyword]))  \n",
    "    # Example: \n",
    "        # text = [[sentence1,sentence2],[sentence_A,sentence_B]]\n",
    "        # keyword = [A]\n",
    "        # Output  = [[0,0],[sentence_A,0]]\n",
    "                                        \n",
    "#f_ScanList = lambda text, keyword: f_SearchKeyword(text, 'keyword') #if (type(text) == str) else \"appel\"\n",
    "#f_ScanList = lambda text, keyword: len(text) if (type(text) == str) else \"appel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e84de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 5644 articles are searched on 33 ontology classes in 0.49408435821533203 seconds.\n",
      "type df_processed to view data frame\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Process Data 1\"\"\"\n",
    "start = time.time() # Measure time\n",
    "\n",
    "# New data frame to store processed data\n",
    "df_processed = df_raw.copy()\n",
    "SourceText = \"Abstract Note\" #Column name text source\n",
    "ProcessedText = \"Clean Abstract Note\" # Column name processed text\n",
    "\n",
    "# clean abstract data \n",
    "df_processed[ProcessedText] = df_raw[SourceText].map(f_CleanText)\n",
    "\n",
    "# function to search data for ontology classes !! depends on df_processed !!\n",
    "f_OntoSearch = lambda onto : list(map(f_ScanList, df_processed[ProcessedText], len(df_processed[ProcessedText])*[onto.lower()]))\n",
    "\n",
    "# Search data ontology class and store in df\n",
    "def f_AddSearchResults(df, ontology):\n",
    "    for onto in ontology: \n",
    "        df[str(onto)] = f_OntoSearch(onto)\n",
    "\n",
    "f_AddSearchResults(df_processed, class_select)\n",
    "\n",
    "end = time.time() # Measure time\n",
    "calc_time = end-start  # Measure time\n",
    "\n",
    "print(\"Your \" + str(len(df_processed)) + \" articles are searched on \"  + str(len(class_select)) + \" ontology classes in \" + str(calc_time) + \" seconds.\")\n",
    "print(\"type df_processed to view data frame\")\n",
    "\n",
    "# Store processed data\n",
    "df_processed.to_csv('ProcessedData_ReadVersion.csv', index = False, encoding='utf-8') #Usefull for looking trough the data\n",
    "# For input in other notebooks, a pickle file is needed to keep data types\n",
    "#df_processed.to_pickle('ProcessedData_ReadVersion.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a08f5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 5644 articles are searched on 33 ontology classes in 0.4173307418823242 seconds.\n",
      "type df_processed to view data frame\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Process data 2\"\"\"\n",
    "\n",
    "# Function Keyword Search    \n",
    "f_SearchKeyword2 = lambda text, keyword : 'y' if (text.find(keyword) != -1) else 'n' \n",
    "    # Searches text for keyword\n",
    "    # Input: text = string or list of strings, keyword = string or list strings\n",
    "    # Example: \n",
    "        # text = [sentence1,sentence2,sentence_A,sentence_B]\n",
    "        # keyword = len(text)*[A]\n",
    "        # Output  = [n,n,y,n]\n",
    "        # Function Scan lists of lists with text\n",
    "f_ScanList2 = lambda text, keyword: list(map(f_SearchKeyword2, text , len(text)*[keyword]))  \n",
    "    # Example: \n",
    "        # text = [[sentence1,sentence2],[sentence_A,sentence_B]]\n",
    "        # keyword = [A]\n",
    "        # Output  = [[n,n],[y,n]]\n",
    "        \n",
    "\"\"\"Process Data\"\"\"\n",
    "start2 = time.time() # Measure time\n",
    "\n",
    "# New data frame to store processed data\n",
    "df_processed2 = df_raw.copy()\n",
    "SourceText = \"Abstract Note\" #Column name text source\n",
    "ProcessedText = \"Clean Abstract Note\" # Column name processed text\n",
    "\n",
    "# clean abstract data \n",
    "df_processed2[ProcessedText] = df_raw[SourceText].map(f_CleanText)\n",
    "\n",
    "# function to search data for ontology classes !! depends on df_processed !!\n",
    "f_OntoSearch = lambda onto : list(map(f_ScanList2, df_processed2[ProcessedText], len(df_processed2[ProcessedText])*[onto.lower()]))\n",
    "\n",
    "# Search data ontology class and store in df\n",
    "def f_AddSearchResults(df, ontology):\n",
    "    for onto in ontology: \n",
    "        df[str(onto)] = f_OntoSearch(onto)\n",
    "\n",
    "f_AddSearchResults(df_processed2, class_select)\n",
    "\n",
    "end2 = time.time() # Measure time\n",
    "calc_time = end2-start2  # Measure time\n",
    "\n",
    "print(\"Your \" + str(len(df_processed2)) + \" articles are searched on \"  + str(len(class_select)) + \" ontology classes in \" + str(calc_time) + \" seconds.\")\n",
    "print(\"type df_processed to view data frame\")\n",
    "\n",
    "# Store processed data for further use.\n",
    "df_processed2.to_csv('ProcessedData.csv', index = False, encoding='utf-8')\n",
    "df_processed2.to_pickle('ProcessedData.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de1ceff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data is processed. processedData.pickle will be used in the data search egine. If you want to read what is in this dataset, open ProcessedData.csv. If you want to know what exactly is found in each article open ProcessedData_Readversion.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"All data is processed. processedData.pickle will be used in the data search engine. If you want to read what is in this dataset, open ProcessedData.csv. If you want to know what exactly is found in each article open ProcessedData_Readversion.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
